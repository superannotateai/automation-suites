{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../docs/sa_logo.png\" width=\"250\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PGnlRWvkY-2c"
   },
   "source": [
    "# Text classification with HuggingFace BERT and SuperAnnotate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial shows an example of solving ```Text classification task``` with [SuperAnnotate](https://www.superannotate.com/) and [HuggingFace](https://huggingface.co/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal of this tutorial is to show how one could annotate some part of data with ```SuperAnnotate``` tools and then build a model with ```HuggingFace``` to automatically annotate the rest of data and upload new annotations to [SuperAnnotate platform](https://app.superannotate.com/). These automatically generated annotations may be additionaly checked and modified manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the experiments described in this tutorial were done with [Ford Sentence Classification](https://www.kaggle.com/datasets/satishkumarmishra/ford-sentence-classifiaction-dataset) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tutorial starts with the assumption that we have partially annotated dataset of image}s.\n",
    "The data is stored on S3 bucket and splitted into two parts: \n",
    "* **train** (~30%) $-$ annotated data for training\n",
    "* **unlabeled** (~70%) $-$ data that will be annotated by the model\n",
    "\n",
    "These folders are connected with existing project on [SuperAnnotate platform](https://app.superannotate.com/) and train dataset has already been annotated manually. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../docs/text_classification_bert/clf_folders.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../docs/text_classification_bert/labeled_texts.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the examples below we used ```SuperAnnotate SDK```, ```Boto3 SDK``` and ```HuggingFace```. $\\ $\n",
    "Some parts of code used here are provided as examples in [SuperAnnotate](https://doc.superannotate.com/docs/getting-started), [Boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) and  [HuggingFace](https://huggingface.co/) documentations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will go through the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{1.}$ [Environmental setup](#envin_setup)\n",
    "\n",
    "\n",
    "$\\textbf{1.1.}$ [User Variables Setup](#user_variables_setup)\n",
    "\n",
    "\n",
    "$\\textbf{1.2.}$ [Constants Setup](#constants_setup)\n",
    "\n",
    "\n",
    "$\\textbf{2.}$ [Load data](#load_data)\n",
    "\n",
    "\n",
    "$\\textbf{3.}$ [Download Labels from SA](#download_labels_from_SA)\n",
    "\n",
    "\n",
    "$\\textbf{4.}$ [Data preprocessing and Dataset implementation](#data_preprocessing)\n",
    "\n",
    "\n",
    "$\\textbf{5.}$ [Text Classification with BERT and Hugging Face](#text_classification)\n",
    "\n",
    "\n",
    "$\\textbf{6.}$ [Train model](#train_model)\n",
    "\n",
    "\n",
    "$\\textbf{7.}$ [Evaluate model](#evaluate_model)\n",
    "       \n",
    "\n",
    "$\\textbf{8.}$ [Get predictions for new texts](#get_predictions)\n",
    "\n",
    "       \n",
    "$\\textbf{9.}$ [Make annotations in SuperAnnotate format](#make_annotations_sa_format)\n",
    "\n",
    "\n",
    "$\\textbf{10.}$ [Upload new annotations to SA](#upload_new_annotations_to_sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "-S077NiGIrid"
   },
   "outputs": [],
   "source": [
    "! pip install superannotate==4.4.7 #SA SDK installation\n",
    "! pip install boto3==1.26.49 # install boto3 client\n",
    "! pip install transformers==4.19.2 # HuggingFace transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.1. User Variables Setup\n",
    "<a id='user_variables_setup'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SuperAnnotate SDK token\n",
    "SA_TOKEN = \"ADD_YOUR_TOKEN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SA_PROJECT_NAME = \"ADD_SUPERANNOTATE_PROJECT_NAME\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Constants Setup\n",
    "<a id='constants_setup'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "from textwrap import wrap\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup, BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from superannotate import SAClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "SuperAnnotate Python SDK functions work within the team scope of the platform, so a team-level authorization is required.\n",
    "\n",
    "To authorize the package in a given team scope, get the authorization token from the team settings page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_client = SAClient(token=SA_TOKEN) ## SuperAnnotate client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data that is shown on SuperAnnotate page is actually stored on AWS S3 Bucket.\n",
    "Here we provide name of this bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"ADD_YOUR_BUCKET_NAME\" # bucket where the data is stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ufzPdoTtNikq"
   },
   "source": [
    "## 2. Load data\n",
    "<a id='load_data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this dict will contain S3 paths to train and unlabeled files\n",
    "data_links_dict = {'train': [],\n",
    "                   'unlabeled': []}\n",
    "\n",
    "#path to the folder containing files in S3 bucket\n",
    "BUCKET_FOLDER_PATH = '/path/to/data/'\n",
    "\n",
    "file_format = '.txt'\n",
    "start_key = ''\n",
    "max_keys = 1000\n",
    "\n",
    "for subset_name in ['train', 'unlabeled']:\n",
    "    while True:\n",
    "        response = s3_client.list_objects_v2(Bucket=BUCKET_NAME,\n",
    "                                             Prefix=f'{BUCKET_FOLDER_PATH}/{subset_name}/',\n",
    "                                             StartAfter=start_key,\n",
    "                                             MaxKeys=max_keys)\n",
    "        objects = response['Contents']\n",
    "        for obj in objects:\n",
    "            path = obj['Key']\n",
    "            if path.endswith(file_format):\n",
    "                data_links_dict[subset_name].append(obj['Key'])\n",
    "        start_key = objects[-1]['Key']\n",
    "        if len(objects) < max_keys:\n",
    "            start_key = ''\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subset_name in subset_names:\n",
    "    print(f\"Loading {subset_name} docs\")\n",
    "    save_dir = f'./{subset_name}_sa_docs'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "    for file_key in tqdm(data_links_dict[subset_name]):\n",
    "        if not '.txt' in file_key:\n",
    "            continue\n",
    "        filename = os.path.basename(file_key)\n",
    "        s3_client.download_file(Bucket=bucket_name, \n",
    "                                Key=file_key,\n",
    "                                Filename=os.path.join(save_dir, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filenames = [os.path.basename(x) for x in data_links_dict['train']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = []\n",
    "for filename in train_filenames:\n",
    "    with open(os.path.join('./train_sa_docs', filename)) as f:\n",
    "        train_texts.append(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jcsWUODBwScb"
   },
   "source": [
    "## 3. Download Labels from SA\n",
    "<a id='download_labels_from_SA'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = sa_client.get_annotations(project=f\"{SA_PROJECT_NAME}/train\", \n",
    "                                        items=train_filenames)\n",
    "\n",
    "labels = [a['instances'][0]['className'] for a in annotations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 506
    },
    "colab_type": "code",
    "id": "Wwh_rW4Efhs3",
    "outputId": "e39b9955-3c5e-45f3-f960-38bfa03447c4"
   },
   "outputs": [],
   "source": [
    "sns.countplot(x=labels)\n",
    "plt.xlabel('Type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9aHyGuTFgyPO"
   },
   "source": [
    "## 4. Data preprocessing and Dataset implementation\n",
    "<a id='data_preprocessing'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NiLb-ltM-ZRz"
   },
   "source": [
    "Upload pre-trained tokenization model [BertTokenizer](https://huggingface.co/transformers/model_doc/bert.html#berttokenizer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E7Mj-0ne--5t"
   },
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H3AfJSZ8NNLF"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kC0YgbzcFvpZ"
   },
   "source": [
    "Set max sequence length equal to 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t7xSmJtLuoxW"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XvvcoU6nurHy"
   },
   "source": [
    "Now we will create PyTorch Dataset. We will use it to train our classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E2BPgRJ7YBK0"
   },
   "outputs": [],
   "source": [
    "class FordDataset(Dataset):\n",
    "\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "  \n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        label = self.labels[item]\n",
    "        encoding = self.tokenizer.encode_plus(text,\n",
    "                                              add_special_tokens=True,\n",
    "                                              max_length=self.max_len,\n",
    "                                              return_token_type_ids=False,\n",
    "                                              padding='max_length',\n",
    "                                              return_attention_mask=True,\n",
    "                                              return_tensors='pt',\n",
    "                                              truncation=True)\n",
    "        return {'text': text,\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'label': torch.tensor(label, dtype=torch.long)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x2uwsvCYqDJK"
   },
   "source": [
    "We will split our labeled dataset into 3 parts: train, validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "labels_int = le.fit_transform(labels)\n",
    "class_names = le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, test_texts, train_labels, test_labels = train_test_split(labeled_texts, labels_int, test_size=0.1)\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(test_texts, test_labels, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J4tQ1x-vqNab"
   },
   "source": [
    "We will create iterators:\n",
    "- train_data_loader - training data\n",
    "- val_data_loader - validation data for training\n",
    "- test_data_loader - data for model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KEGqcvkuOuTX"
   },
   "outputs": [],
   "source": [
    "def create_data_loader(texts, labels, tokenizer, max_len, batch_size):\n",
    "    ds = FordDataset(texts=texts,\n",
    "                     labels=labels,\n",
    "                     tokenizer=tokenizer,\n",
    "                     max_len=max_len)\n",
    "    return DataLoader(ds, batch_size=batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vODDxMKsPHqI"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "train_data_loader = create_data_loader(train_texts, \n",
    "                                       train_labels, \n",
    "                                       tokenizer, \n",
    "                                       MAX_LEN, \n",
    "                                       BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(val_texts, \n",
    "                                     val_labels, \n",
    "                                     tokenizer, \n",
    "                                     MAX_LEN, \n",
    "                                     BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(test_texts, \n",
    "                                      test_labels, \n",
    "                                      tokenizer, \n",
    "                                      MAX_LEN, \n",
    "                                      BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H63Y-TjyRC7S"
   },
   "source": [
    "## 5. Text Classification with BERT and Hugging Face\n",
    "<a id='text_classification'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "440Nd31VTHER"
   },
   "source": [
    "\n",
    "We will use basic [BertModel](https://huggingface.co/transformers/model_doc/bert.html#bertmodel) and implement text classifier based on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0P41FayISNRI"
   },
   "outputs": [],
   "source": [
    "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0o_NiS3WgOFf"
   },
   "source": [
    "Now we can implement our text classifier based on BertModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m_mRflxPl32F"
   },
   "outputs": [],
   "source": [
    "class SentimentClassifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "        self.drop = torch.nn.Dropout(p=0.3)\n",
    "        self.out = torch.nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "  \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        bert_output = self.bert(input_ids=input_ids,\n",
    "                                     attention_mask=attention_mask)\n",
    "        \n",
    "        pooled_output = bert_output[1]\n",
    "        output = self.drop(pooled_output)\n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i0yQnuSFsjDp"
   },
   "outputs": [],
   "source": [
    "model = SentimentClassifier(len(class_names))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g9xikRdtRN1N"
   },
   "source": [
    "## 6. Train model\n",
    "<a id='train_model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5v-ArJ2fCCcU"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 3\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A8522g7JIu5J"
   },
   "source": [
    "Implement a function for one epoch of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bzl9UhuNx1_Q"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, \n",
    "                data_loader, \n",
    "                loss_fn, \n",
    "                optimizer, \n",
    "                device, \n",
    "                scheduler, \n",
    "                n_examples):\n",
    "    model = model.train()\n",
    "\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "  \n",
    "    for d in tqdm(data_loader):\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        labels = d[\"label\"].to(device)\n",
    "        outputs = model(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask)\n",
    "\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E4PniYIte0fr"
   },
   "source": [
    "We should also implement model evalutation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CXeRorVGIKre"
   },
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    "\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in tqdm(data_loader):\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            labels = d[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a_rdSDBHhhCh"
   },
   "source": [
    "Now we could implement model training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 901
    },
    "colab_type": "code",
    "id": "1zhHoFNsxufs",
    "outputId": "2f11710a-700e-4933-b57e-5d50e5ed1f78"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    train_acc, train_loss = train_epoch(model,\n",
    "                                        train_data_loader,    \n",
    "                                        loss_fn, \n",
    "                                        optimizer, \n",
    "                                        device, \n",
    "                                        scheduler, \n",
    "                                        len(train_texts))\n",
    "\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "    val_acc, val_loss = eval_model(model,\n",
    "                                   val_data_loader,\n",
    "                                   loss_fn, \n",
    "                                   device, \n",
    "                                   len(val_texts))\n",
    "\n",
    "    print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "    print()\n",
    "\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "    if val_acc > best_accuracy:\n",
    "        torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "        best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kU9MfvTQC9wh"
   },
   "source": [
    "## 7. Evaluate model\n",
    "<a id='evaluate_model'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s1z8MZ3GDZsl"
   },
   "source": [
    "We can now implement function to get the prediction from the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EgR6MuNS8jr_"
   },
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "    model = model.eval()\n",
    "  \n",
    "    review_texts = []\n",
    "    predictions = []\n",
    "    prediction_probs = []\n",
    "    real_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in tqdm(data_loader):\n",
    "\n",
    "            texts = d[\"text\"]\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            labels = d[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "            review_texts.extend(texts)\n",
    "            predictions.extend(preds)\n",
    "            prediction_probs.extend(probs)\n",
    "            real_values.extend(labels)\n",
    "\n",
    "    predictions = torch.stack(predictions).cpu()\n",
    "    prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "    real_values = torch.stack(real_values).cpu()\n",
    "    return review_texts, predictions, prediction_probs, real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zHdPZr60-0c_"
   },
   "outputs": [],
   "source": [
    "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(model,\n",
    "                                                               test_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rnVDSjRyCK68"
   },
   "source": [
    "Let's estimate model's performance on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "L8a9_8-ND3Is",
    "outputId": "9b2c48cc-b62e-41f3-dba5-af90457a37de"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4sCG_6yOCHdU"
   },
   "source": [
    "We can look at confusion matrix of predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 533
    },
    "colab_type": "code",
    "id": "6d1qxsc__DTh",
    "outputId": "14b8839c-4e14-430c-b185-46b09bd4231e"
   },
   "outputs": [],
   "source": [
    "def show_confusion_matrix(confusion_matrix):\n",
    "    hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "    hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
    "    plt.ylabel('True class')\n",
    "    plt.xlabel('Predicted class');\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "show_confusion_matrix(df_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7WL5pDmvFyaU"
   },
   "source": [
    "## 8. Get predictions for new texts\n",
    "<a id='get_predictions'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G1cg2r4SvLrT"
   },
   "source": [
    "Now we can get prediction for any new text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_texts = []\n",
    "for filename in train_filenames:\n",
    "    with open(os.path.join('./unlabeled_sa_docs', filename)) as f:\n",
    "        train_texts.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one_text(text, model, tokenizer, class_names):\n",
    "    encoded_review = tokenizer.encode_plus(text,\n",
    "                                           max_length=MAX_LEN,\n",
    "                                           add_special_tokens=True,\n",
    "                                           return_token_type_ids=False,\n",
    "                                           padding='max_length',\n",
    "                                           return_tensors='pt',\n",
    "                                           truncation=True,\n",
    "                                           return_attention_mask=True)\n",
    "    input_ids = encoded_review['input_ids'].to(device)\n",
    "    attention_mask = encoded_review['attention_mask'].to(device)\n",
    "    output = model(input_ids, attention_mask)\n",
    "    _, prediction = torch.max(output, dim=1)\n",
    "    return class_names[prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels = [predict_one_text(text, model, tokenizer, class_names) for text in tqdm(unlabeld_texts)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Make annotations in SuperAnnotate format\n",
    "<a id='make_annotations_sa_format'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on predictions made by the model we should now create annotations in SuperAnnotate format to be able to upload them to SuperAnnotate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTATIONS_FOLDER = \"PUT_FOLDER_FOR_NEW_ANNOTATIONS_HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label,filename in zip(new_labels, filenames):\n",
    "    annotation = {\"metadata\": {\"name\": filename},\n",
    "                  \"instances\": [{\"type\": \"tag\",\n",
    "                                 \"className\": file2type[filename],\n",
    "                                 \"attributes\": []}]}\n",
    "    with open(os.path.join(ANNOTATIONS_FOLDER,filename)) as f:\n",
    "        json.dump(annotation,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Upload new annotations to SA \n",
    "<a id='upload_new_annotations_to_sa'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now we could upload annotations generated on the previous step back to SuperAnnnotate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(filename):\n",
    "    with open(filename) as f:\n",
    "        data = json.load(f)\n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "files = os.listdir(ANNOTATIONS_FOLDER)\n",
    "files_per_step = 500\n",
    "steps = len(files) // files_per_step + 1\n",
    "\n",
    "for step in range(steps):\n",
    "    start = step * files_per_step\n",
    "    end = min((step + 1)* files_per_step, len(files))\n",
    "\n",
    "    batch = [read_json_file(os.path.join(ANNOTATIONS_FOLDER, f)) for f in files[start: end]]\n",
    "\n",
    "    outputs.append(sa_client.upload_annotations(project=f'{SA_PROJECT_NAME}/unlabeled/', annotations=batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at unlabeled folder at the SuperAnnotate page and see the predictions made by our model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All files in unlabeled folder changed their status."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../docs/text_classification_bert/train_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can open any of these files and check whether it is annitated correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../docs/text_classification_bert/text_example.png)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Копия блокнота \"08.sentiment-analysis-with-bert.ipynb\"",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
