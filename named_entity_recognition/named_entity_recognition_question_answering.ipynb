{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "debab20c-466d-4034-b170-4734f851051a",
   "metadata": {},
   "source": [
    "<img src=\"../docs/sa_logo.png\" width=\"250\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bd5c92-b88a-4975-befa-ee2e84675e7b",
   "metadata": {
    "colab_type": "text",
    "id": "PGnlRWvkY-2c"
   },
   "source": [
    "# Question Answering NER with HuggingFace "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0adbbe-3f0d-4c93-ace8-5a64b83811df",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d890e68-1417-4b8f-91f8-71d111b210ae",
   "metadata": {},
   "source": [
    "This tutorial shows an example of solving ```Named Entity Recognition task``` with [SuperAnnotate](https://www.superannotate.com/) and [HuggingFace](https://huggingface.co/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40de6662-f702-4831-8731-30e2e923d673",
   "metadata": {},
   "source": [
    "The main goal of this tutorial is to show how one could annotate some part of data with ```SuperAnnotate``` tools and then build a model with ```HuggingFace``` to automatically annotate the rest of data and upload new annotations to [SuperAnnotate platform](https://app.superannotate.com/). These automatically generated annotations may be additionaly checked and modified manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31799dad-814b-470b-9eb2-0c46f190fcba",
   "metadata": {},
   "source": [
    "All the experiments described in this tutorial were done with [Legal NER](https://paperswithcode.com/dataset/legal-ner) dataset. It is a corpus of 46545 annotated legal named entities mapped to 14 legal entity types. It is designed for named entity recognition in indian court judgement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9332327",
   "metadata": {},
   "source": [
    "![](../docs/legal-ner/folders_legal_ner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51494c70-acf6-42c3-b7fb-26e400670a2b",
   "metadata": {},
   "source": [
    "The tutorial starts with the assumption that we have partially annotated dataset of texts.\n",
    "The data is stored on S3 bucket and splitted into two parts: \n",
    "* **train** (~40%) $-$ annotated data for training\n",
    "* **unlabeled** (~60%) $-$ data that will be annotated by the model\n",
    "\n",
    "These folders are connected with existing project on [SuperAnnotate platform](https://app.superannotate.com/) and train dataset has already been annotated manually. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d9eca6",
   "metadata": {},
   "source": [
    "![](../docs/legal-ner/new_lner_example_train.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc6e1c5-ad12-43ca-a0f8-82abec50f3d7",
   "metadata": {},
   "source": [
    "In the examples below we used ```SuperAnnotate SDK```, ```Boto3 SDK``` and ```HuggingFace```. $\\ $\n",
    "Some parts of code used here are provided as examples in [SuperAnnotate](https://doc.superannotate.com/docs/getting-started), [Boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) and  [HuggingFace](https://huggingface.co/) documentations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf375b5",
   "metadata": {},
   "source": [
    "In this tutorial we will solve Named Entity Recognition problem as Question Answering problem. The algorithm we will use was introduced in [QaNER: Prompting Question Answering Models for Few-shot Named Entity Recognitcon](https://arxiv.org/abs/2203.01543)\n",
    "\n",
    "Some parts of code in the tutorial are based on [this unofficial implementation](https://github.com/dayyass/QaNER) of QaNER algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95907ec6-755e-47bb-a7b2-388a19f9e50c",
   "metadata": {},
   "source": [
    "In this tutorial we will go through the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0fbc97",
   "metadata": {},
   "source": [
    "$\\textbf{1.}$ [Environmental setup](#environmental_setup)\n",
    "\n",
    "$\\textbf{1.1}$ [User Variables Setup](#user_variables)\n",
    "\n",
    "$\\textbf{1.1}$ [Constants Setup](#constants_setup)\n",
    "\n",
    "$\\textbf{2.}$ [Download documents and labels from SuperAnnotate](#download_data)\n",
    "\n",
    "$\\textbf{2.1}$ [Get links to all files in S3 bucket](#list_all_files_s3)\n",
    "\n",
    "$\\textbf{2.2}$ [Download files](#download_files)\n",
    "\n",
    "$\\textbf{2.3}$ [Download labels from SuperAnnotate](#download_labels_from_sa)\n",
    "   \n",
    "$\\textbf{3.}$ [Prepare data for Bert NER model](#prepare_data_for_bert_model)\n",
    "\n",
    "$\\textbf{4.}$ [Train model](#train_model)\n",
    "\n",
    "$\\textbf{5.}$ [Get predictions for unlabeled texts](#get_predictions_for_unlabeled_texts)\n",
    "\n",
    "$\\textbf{6.}$ [Make annotations in SuperAnnotate format](#make_annotations_sa_format)\n",
    "\n",
    "$\\textbf{7.}$ [Upload new annotations to SuperAnnotate platform](#upload_new_annotations_to_sa_platform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c410908-a26b-426e-bc9d-f6e610f50819",
   "metadata": {},
   "source": [
    "## 1. Environmental setup\n",
    "<a id='environmental_setup'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018cf41e",
   "metadata": {},
   "source": [
    "First we will install dependencies we will need further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeca537e-ba33-4389-a5ba-bbc97b062515",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install superannotate==4.4.7 #SA SDK installation\n",
    "! pip install boto3 # install boto3 client\n",
    "! pip install transformers # HuggingFace transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6dae51-a536-4190-9c01-d03dc8382fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from superannotate import SAClient\n",
    "from transformers import BertTokenizerFast\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD, Adam, NAdam\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertForTokenClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6568072d-a330-488d-9630-f34972afef28",
   "metadata": {},
   "source": [
    "### 1.1 User Variables Setup\n",
    "<a id='user_variables'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ce5f6f",
   "metadata": {},
   "source": [
    "Here we should add an [access token for SuperAnnotate SDK](https://doc.superannotate.com/docs/token-for-python-sdk) and the project name for the project with our annotated files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bca3da4-3b45-41bf-af85-641319925f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SuperAnnotate SDK token\n",
    "SA_TOKEN = \"ADD_YOUR_TOKEN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64b6c4e-f744-4202-9221-4fac99a09e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SA_PROJECT_NAME = \"ADD_SUPERANNOTATE_PROJECT_NAME\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e907de6f-8e3d-4713-88fe-8a224669b6c2",
   "metadata": {},
   "source": [
    "### 1.2 Constants Setup\n",
    "<a id='constants_setup'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e299497-b21e-4fc8-8839-9b36b2254126",
   "metadata": {
    "tags": []
   },
   "source": [
    "SuperAnnotate Python SDK functions work within the team scope of the platform, so a team-level authorization is required.\n",
    "\n",
    "To authorize the package in a given team scope, get the authorization token from the team settings page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460f2a8a-640e-4421-92fa-98448985bc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_client = SAClient(token=SA_TOKEN) ## SuperAnnotate client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1486ae6e",
   "metadata": {},
   "source": [
    "## 2. Download documents and labels from SuperAnnotate\n",
    "<a id='download_data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178726a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "bucket_name = 'sa-public-datasets'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfa6805-b1e5-48ea-a4e0-e58963846380",
   "metadata": {},
   "source": [
    "Data that is shown on SuperAnnotate page is actually stored on AWS S3 Bucket.\n",
    "Here we provide name of this bucket.\n",
    "\n",
    "We will later download these files from the bucket to use them for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65c0bff-ba69-47ac-8b2f-79b711cd84da",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"ADD_YOUR_BUCKET_NAME\" # bucket where the data is stored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfbb793-7728-4722-acce-ef35f4b55ca2",
   "metadata": {},
   "source": [
    "We should also create client to be able to work with AWS S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83107e9-8496-4512-b98e-5d99871c7585",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3') ## S3 client\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e23db5",
   "metadata": {},
   "source": [
    "### 2.1. Get links to all files in S3 bucket\n",
    "<a id='list_all_files_s3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b37f9f-3474-4f8c-97b4-6668ae1ca816",
   "metadata": {},
   "source": [
    "Texts shown on SuperAnnotate page are stored in S3 bucket.\n",
    "We can download them to local computer and train our model for legal entities recognition.\n",
    "\n",
    "Before that we should get links to all of them.\n",
    "Since S3 SDK could list only 1000 objects per step, we could do it iteratively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80382016",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_names = ['train', 'unlabeled']\n",
    "\n",
    "data_links_dict = {'train': [],\n",
    "                   'unlabeled': []}\n",
    "\n",
    "BUCKET_FOLDER_PATH = '/path/to/data/'\n",
    "\n",
    "start_key = ''\n",
    "\n",
    "for subset_name in subset_names:\n",
    "    print(\"Processing\", subset_name)\n",
    "    while True:\n",
    "        response = s3_client.list_objects_v2(Bucket=bucket_name,\n",
    "                                             Prefix=f'{BUCKET_FOLDER_PATH}/{subset_name}/',\n",
    "                                             StartAfter=start_key)\n",
    "        objects = response['Contents']\n",
    "        for obj in objects:\n",
    "            data_links_dict[subset_name].append(obj['Key'])\n",
    "        print(f\"\\t{len(data_links_dict[subset_name])} files in {subset_name}\")\n",
    "        start_key = objects[-1]['Key']\n",
    "        if len(objects) < 1000:\n",
    "            start_key = ''\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c191650a",
   "metadata": {},
   "source": [
    "### 2.2. Download files\n",
    "<a id='download_files'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892942fe-9b07-4e01-b0d1-b8c4b60920fb",
   "metadata": {},
   "source": [
    "Now we will use these links to download all the files from S3 bucket.\n",
    "\n",
    "We can do it via *download_file* function from *boto3* SDK. \n",
    "\n",
    "After downloading each file we will store it in *save_dir* folder on our local computer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216dfcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_format = '.txt'\n",
    "\n",
    "for subset_name in subset_names:\n",
    "    print(f\"Loading {subset_name} docs\")\n",
    "    save_dir = f'./{subset_name}_sa_docs' # name for the folder to store downloaded images\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "    for file_key in tqdm(data_links_dict[subset_name]):\n",
    "        if not file_key.endswith(file_format):\n",
    "            continue\n",
    "        filename = os.path.basename(file_key)\n",
    "        s3_client.download_file(Bucket=bucket_name, \n",
    "                                Key=file_key,\n",
    "                                Filename=os.path.join(save_dir, filename))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e36049c",
   "metadata": {},
   "source": [
    "### 2.3 Download labels from SuperAnnotate\n",
    "<a id='download_labels_from_sa'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c569da6-7eb9-4a62-afef-8764aa2fce1a",
   "metadata": {},
   "source": [
    "Now we can download labels from SuperAnnotate for the train texts that were annotated manually. The annotations will be downloaded in [SuperAnnotate format](https://doc.superannotate.com/docs/sdk-export-annotations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff0e565",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"PUT_YOUR_TOKEN_HERE\"\n",
    "\n",
    "sa_client = SAClient(token = token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13e258f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_response = sa_client.get_annotations(project=SA_PROJECT_NAME,\n",
    "                                        items=[os.path.basename(x) for x \\\n",
    "                                               in data_links_dict['train']])\n",
    "\n",
    "annotations = [i['instances'] for i in sa_response]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168b2b99",
   "metadata": {},
   "source": [
    "Here we will define the *PROMPT_MAPPER* dictionary that will be used later to build the questions for our Question Answering NER model.\n",
    "\n",
    "The keys of the dictionary $-$ legal entities names and the values $-$ phares that will be inserted into questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d74089",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_MAPPER = {'CASE_NUMBER': 'case number',\n",
    "                 'COURT': 'court',\n",
    "                 'DATE':'date',\n",
    "                 'GPE':'location',\n",
    "                 'JUDGE':'judge',\n",
    "                 'LAWYER':'lawyer',\n",
    "                 'ORG':'organization',\n",
    "                 'OTHER_PERSON':'other person',\n",
    "                 'PETITIONER':'petitioner',\n",
    "                 'PRECEDENT':'precedent',\n",
    "                 'PROVISION':'provision',\n",
    "                 'RESPONDENT':'respondent',\n",
    "                 'STATUTE':'statute',\n",
    "                 'WITNESS':'witness'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8149d73e",
   "metadata": {},
   "source": [
    "## 3. Prepare data for Bert NER model\n",
    "<a id='prepare_data_for_bert_model'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7125cf-7d46-4ff6-b111-35112c645146",
   "metadata": {},
   "source": [
    "We will use pretrained tokenizer *bert-base-cased* for our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142f392e-e19a-45c5-a458-d3f9003b78ee",
   "metadata": {},
   "source": [
    "Now we can create class for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad33fc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 qa_sentences,\n",
    "                 qa_labels,\n",
    "                 prompt_mapper):\n",
    "        super().__init__()\n",
    "        self.prompt_mapper = prompt_mapper\n",
    "        self.dataset = self._prepare_dataset(qa_sentences=qa_sentences,\n",
    "                                             qa_labels=qa_labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "    def _prepare_dataset(self,\n",
    "                         qa_sentences,\n",
    "                         qa_labels):\n",
    "\n",
    "        dataset = []\n",
    "        for sentence, labels in tqdm(zip(qa_sentences, qa_labels), desc=\"prepare_dataset\"):\n",
    "            for label_tag, label_name in self.prompt_mapper.items():\n",
    "                question_prompt = f\"What is the {label_name}?\"\n",
    "\n",
    "                answer_list = []\n",
    "                for span in labels:\n",
    "                    if span['label'] == label_tag:\n",
    "                        answer_list.append(span)\n",
    "\n",
    "                if len(answer_list) == 0:\n",
    "                    empty_span = {\"token\" : \"\",\n",
    "                                  \"label\" : \"O\",\n",
    "                                  \"start_context_char_pos\" : 0,\n",
    "                                  \"end_context_char_pos\" : 0}\n",
    "                    instance = {'context' : sentence,\n",
    "                                'question' : question_prompt,\n",
    "                                'answer' : empty_span}\n",
    "                    dataset.append(instance)\n",
    "                else:\n",
    "                    for answer in answer_list:\n",
    "                        instance = {'context' : sentence,\n",
    "                                    'question' : question_prompt,\n",
    "                                    'answer' : answer}\n",
    "                        dataset.append(instance)\n",
    "\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22755592",
   "metadata": {},
   "source": [
    "In our case is also useful to create a Collator class to prepare data for QA model. It will use *Tokenizer* model from *transformers* and then transform its output. More info about data processing for Question Answering could be found in [HuggingFace docs ](https://huggingface.co/docs/transformers/tasks/question_answering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789fc352",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class Collator:\n",
    "    def __init__(self, tokenizer, tokenizer_kwargs):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer_kwargs = tokenizer_kwargs\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        context_list = []\n",
    "        question_list = []\n",
    "        start_end_context_char_pos_list = []\n",
    "\n",
    "        for instance in batch:\n",
    "            context_list.append(instance['context'])\n",
    "            question_list.append(instance['question'])\n",
    "            start_end_context_char_pos_list.append(\n",
    "                [\n",
    "                    instance['answer']['start_context_char_pos'],\n",
    "                    instance['answer']['end_context_char_pos'],\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        tokenized_batch = self.tokenizer(question_list, context_list, **self.tokenizer_kwargs)\n",
    "\n",
    "        offset_mapping_batch = tokenized_batch[\"offset_mapping\"].numpy().tolist()\n",
    "\n",
    "        assert len(offset_mapping_batch) == len(start_end_context_char_pos_list)\n",
    "\n",
    "        start_tok_pos_list, end_tok_pos_list = [], []\n",
    "        for offset_mapping, start_end_char_pos in zip(offset_mapping_batch, start_end_context_char_pos_list):\n",
    "            if start_end_char_pos == [0, 0]:\n",
    "                start_tok_pos_list.append(0)\n",
    "                end_tok_pos_list.append(0)\n",
    "            else:\n",
    "                start_tok_pos, end_tok_pos = self.ch_to_tok_bounds(offset_mapping=offset_mapping,\n",
    "                                                                   start_end_char_pos=start_end_char_pos)\n",
    "                start_tok_pos_list.append(start_tok_pos)\n",
    "                end_tok_pos_list.append(end_tok_pos)\n",
    "\n",
    "        tokenized_batch[\"start_positions\"] = torch.LongTensor(start_tok_pos_list)\n",
    "        tokenized_batch[\"end_positions\"] = torch.LongTensor(end_tok_pos_list)\n",
    "\n",
    "        tokenized_batch[\"instances\"] = batch\n",
    "\n",
    "        return tokenized_batch\n",
    "\n",
    "    @staticmethod\n",
    "    def ch_to_tok_bounds(offset_mapping, start_end_char_pos):\n",
    "        start_context_char_pos, end_context_char_pos = start_end_char_pos\n",
    "        assert end_context_char_pos >= start_context_char_pos\n",
    "        \n",
    "        done = False\n",
    "        special_tokens_cnt = 0\n",
    "        for i, token_boundaries in enumerate(offset_mapping):\n",
    "            if token_boundaries == [0, 0]:\n",
    "                special_tokens_cnt += 1\n",
    "                continue\n",
    "            if special_tokens_cnt == 2:\n",
    "                start_token_pos, end_token_pos = token_boundaries\n",
    "                if start_token_pos == start_context_char_pos:\n",
    "                    res_start_token_pos = i\n",
    "                if end_token_pos == end_context_char_pos:\n",
    "                    res_end_token_pos = i  \n",
    "                    done = True\n",
    "                    break\n",
    "        if special_tokens_cnt > 2:\n",
    "            res_end_token_pos = len(offset_mapping) - 1  \n",
    "            res_start_token_pos = 0\n",
    "            done = True\n",
    "            \n",
    "        assert done\n",
    "        return res_start_token_pos, res_end_token_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d92943b-a79e-45fa-89fa-fdc20ce40ca9",
   "metadata": {},
   "source": [
    "Now we upload train texts that we downloaded from S3 bucket and split them into train and test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c19794",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DOCS_FOLDER = f'./train_sa_docs'\n",
    "\n",
    "texts = []\n",
    "\n",
    "for path in data_links_dict['train']:\n",
    "    filepath = os.path.join(TRAIN_DOCS_FOLDER,os.path.basename(path))\n",
    "    with open(filepath) as f:\n",
    "        texts.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c943506",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_qa_sents, test_qa_sents, train_qa_labels, test_qa_labels = train_test_split(texts, spans, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283d06e8",
   "metadata": {},
   "source": [
    "## 4. Train model\n",
    "<a id='train_model'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eb6b62",
   "metadata": {},
   "source": [
    "Now we can actually create the datasets for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44eccbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = QADataset(qa_sentences=train_qa_sents,\n",
    "                          qa_labels=train_qa_labels,\n",
    "                          prompt_mapper=PROMPT_MAPPER)\n",
    "\n",
    "\n",
    "test_dataset = QADataset(qa_sentences=test_qa_sents,\n",
    "                         qa_labels=test_qa_labels,\n",
    "                         prompt_mapper=PROMPT_MAPPER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2ee5aa-24e3-47ac-b2fd-80238b43d5a6",
   "metadata": {},
   "source": [
    "Let's now implement the training loop and all the functions we need for it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddaa5f8",
   "metadata": {},
   "source": [
    "In order to be able to evaluate the model after each epoch of training we will implement the function to compute span-wise precision and recall metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183a6d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(spans_true_batch,\n",
    "                    spans_pred_batch_top_1,\n",
    "                    prompt_mapper): \n",
    "    \n",
    "    metrics = {}\n",
    "\n",
    "    entity_mapper = {\"O\": 0}\n",
    "    for entity_tag in prompt_mapper:\n",
    "        entity_mapper[entity_tag] = len(entity_mapper)\n",
    "\n",
    "    ner_confusion_matrix = np.zeros((len(entity_mapper), len(entity_mapper)))\n",
    "    confusion_matrix_true_denominator = np.zeros(len(entity_mapper))\n",
    "    confusion_matrix_pred_denominator = np.zeros(len(entity_mapper))\n",
    "\n",
    "    for span_true, span_pred in zip(spans_true_batch, spans_pred_batch_top_1):\n",
    "        span_pred = span_pred[0]\n",
    "        i = entity_mapper[span_true['label']]\n",
    "        j = entity_mapper[span_pred['label']]\n",
    "        confusion_matrix_true_denominator[i] += 1\n",
    "        confusion_matrix_pred_denominator[j] += 1\n",
    "        if span_true == span_pred:\n",
    "            ner_confusion_matrix[i, j] += 1\n",
    "            \n",
    "    assert (confusion_matrix_true_denominator.sum() == confusion_matrix_pred_denominator.sum())\n",
    "\n",
    "    ner_confusion_matrix_diag = np.diag(ner_confusion_matrix)\n",
    "\n",
    "    accuracy = np.nan_to_num(ner_confusion_matrix_diag.sum() / confusion_matrix_true_denominator.sum())\n",
    "    precision_per_entity_type = np.nan_to_num(ner_confusion_matrix_diag / confusion_matrix_pred_denominator)\n",
    "    recall_per_entity_type = np.nan_to_num(ner_confusion_matrix_diag / confusion_matrix_true_denominator)\n",
    "    f1_per_entity_type = np.nan_to_num(2 * precision_per_entity_type * recall_per_entity_type\n",
    "                                        / (precision_per_entity_type + recall_per_entity_type))\n",
    "\n",
    "    metrics[\"accuracy\"] = accuracy\n",
    "\n",
    "    for label_tag, idx in entity_mapper.items():\n",
    "        metrics[f\"precision_tag_{label_tag}\"] = precision_per_entity_type[idx]\n",
    "        metrics[f\"recall_tag_{label_tag}\"] = recall_per_entity_type[idx]\n",
    "        metrics[f\"f1_tag_{label_tag}\"] = f1_per_entity_type[idx]\n",
    "\n",
    "    metrics[\"precision_macro\"] = precision_per_entity_type.mean()\n",
    "    metrics[\"recall_macro\"] = recall_per_entity_type.mean()\n",
    "    metrics[\"f1_macro\"] = f1_per_entity_type.mean()\n",
    "\n",
    "    metrics[\"precision_weighted\"] = np.average(precision_per_entity_type,\n",
    "                                               weights=confusion_matrix_true_denominator)\n",
    "    metrics[\"recall_weighted\"] = np.average(recall_per_entity_type,\n",
    "                                            weights=confusion_matrix_true_denominator)\n",
    "    metrics[\"f1_weighted\"] = np.average(f1_per_entity_type,\n",
    "                                        weights=confusion_matrix_true_denominator)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa45ab7f",
   "metadata": {},
   "source": [
    "We also need a function to select top valid spans according to the logits produced by our QaNER model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48222fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_valid_spans(context_list,\n",
    "                        question_list,\n",
    "                        prompt_mapper,\n",
    "                        inputs,\n",
    "                        outputs,\n",
    "                        offset_mapping_batch,\n",
    "                        n_best_size = 1,\n",
    "                        max_answer_length = 100):\n",
    "\n",
    "    batch_size = len(offset_mapping_batch)\n",
    "\n",
    "    inv_prompt_mapper = {v: k for k, v in prompt_mapper.items()}\n",
    "\n",
    "    assert batch_size == len(context_list)\n",
    "    assert batch_size == len(question_list)\n",
    "    assert batch_size == len(inputs[\"input_ids\"])\n",
    "    assert batch_size == len(inputs[\"token_type_ids\"])\n",
    "    assert batch_size == len(outputs[\"start_logits\"])\n",
    "    assert batch_size == len(outputs[\"end_logits\"])\n",
    "\n",
    "    top_valid_spans_batch = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        context = context_list[i]\n",
    "\n",
    "        offset_mapping = offset_mapping_batch[i].cpu().numpy()\n",
    "        mask = inputs[\"token_type_ids\"][i].bool().cpu().numpy()\n",
    "        offset_mapping[~mask] = [0, 0]\n",
    "        offset_mapping = [\n",
    "            (span if span != [0, 0] else None) for span in offset_mapping.tolist()\n",
    "        ]\n",
    "\n",
    "        start_logits = outputs[\"start_logits\"][i].cpu().numpy()\n",
    "        end_logits = outputs[\"end_logits\"][i].cpu().numpy()\n",
    "\n",
    "        start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "        end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "        top_valid_spans = []\n",
    "\n",
    "        for start_index, end_index in zip(start_indexes, end_indexes):\n",
    "            if (start_index >= len(offset_mapping)\n",
    "                or end_index >= len(offset_mapping)\n",
    "                or offset_mapping[start_index] is None\n",
    "                or offset_mapping[end_index] is None\n",
    "            ):\n",
    "                continue\n",
    "            if (end_index < start_index) or (end_index - start_index + 1 > max_answer_length):\n",
    "                continue\n",
    "            if start_index <= end_index:\n",
    "                start_context_char_char, end_context_char_char = offset_mapping[start_index]\n",
    "                span = {\"token\" : context[start_context_char_char:end_context_char_char],\n",
    "                        \"label\" : inv_prompt_mapper[question_list[i].split(r\"What is the \")[-1].rstrip(r\"?\")],\n",
    "                        \"start_context_char_pos\" : start_context_char_char,\n",
    "                        \"end_context_char_pos\" : end_context_char_char}\n",
    "                top_valid_spans.append(span)\n",
    "        top_valid_spans_batch.append(top_valid_spans)\n",
    "    return top_valid_spans_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1769615d",
   "metadata": {},
   "source": [
    "We can now implement the functions for one epoch of training and for evaluating model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c5e3ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_epoch(model,\n",
    "                dataloader,\n",
    "                optimizer,\n",
    "                device,\n",
    "                epoch):\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    batch_metrics_list = defaultdict(list)\n",
    "    for i, inputs in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        instances_batch = inputs.pop(\"instances\")\n",
    "        context_list, question_list = [], []\n",
    "        for instance in instances_batch:\n",
    "            context_list.append(instance['context'])\n",
    "            question_list.append(instances['question'])\n",
    "        inputs = inputs.to(device)\n",
    "        offset_mapping_batch = inputs.pop(\"offset_mapping\")\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss.append(loss.item())\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            outputs_inference = model(**inputs)\n",
    "            model.train()\n",
    "        spans_pred_batch_top_1 = get_top_valid_spans(context_list=context_list,\n",
    "                                                     question_list=question_list,\n",
    "                                                     prompt_mapper=dataloader.dataset.prompt_mapper,\n",
    "                                                     inputs=inputs,\n",
    "                                                     outputs=outputs_inference,\n",
    "                                                     offset_mapping_batch=offset_mapping_batch,\n",
    "                                                     n_best_size=1,\n",
    "                                                     max_answer_length=100)\n",
    "\n",
    "        for idx in range(len(spans_pred_batch_top_1)):\n",
    "            if not spans_pred_batch_top_1[idx]:\n",
    "                empty_span = {\"token\" : \"\",\n",
    "                              \"label\" : \"O\",\n",
    "                              \"start_context_char_pos\" : 0,\n",
    "                              \"end_context_char_pos\" : 0}\n",
    "                spans_pred_batch_top_1[idx] = [empty_span]\n",
    "\n",
    "        spans_true_batch = [instance['answer'] for instance in instances_batch]\n",
    "\n",
    "        batch_metrics = compute_metrics(spans_true_batch=spans_true_batch,\n",
    "                                        spans_pred_batch_top_1=spans_pred_batch_top_1,\n",
    "                                        prompt_mapper=dataloader.dataset.prompt_mapper)\n",
    "\n",
    "        for metric_name, metric_value in batch_metrics.items():\n",
    "            batch_metrics_list[metric_name].append(metric_value)\n",
    "\n",
    "    avg_loss = np.mean(epoch_loss)\n",
    "    print(f\"Train loss: {avg_loss}\\n\")\n",
    "\n",
    "    for metric_name, metric_value_list in batch_metrics_list.items():\n",
    "        metric_value = np.mean(metric_value_list)\n",
    "        print(f\"Train {metric_name}: {metric_value}\\n\")\n",
    "\n",
    "\n",
    "def evaluate_epoch(model, dataloader, device, epoch):\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = []\n",
    "    batch_metrics_list = defaultdict(list)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, inputs in tqdm(enumerate(dataloader),total=len(dataloader)):\n",
    "\n",
    "            instances_batch = inputs.pop(\"instances\")\n",
    "\n",
    "            context_list, question_list = [], []\n",
    "            for instance in instances_batch:\n",
    "                context_list.append(instance['context'])\n",
    "                question_list.append(instance['question'])\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            offset_mapping_batch = inputs.pop(\"offset_mapping\")\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            epoch_loss.append(loss.item())\n",
    "            spans_pred_batch_top_1 = get_top_valid_spans(context_list=context_list,\n",
    "                                                         question_list=question_list,\n",
    "                                                         prompt_mapper=dataloader.dataset.prompt_mapper,\n",
    "                                                         inputs=inputs,\n",
    "                                                         outputs=outputs,\n",
    "                                                         offset_mapping_batch=offset_mapping_batch,\n",
    "                                                         n_best_size=1,\n",
    "                                                         max_answer_length=100)\n",
    "\n",
    "            for idx in range(len(spans_pred_batch_top_1)):\n",
    "                if not spans_pred_batch_top_1[idx]:\n",
    "                    empty_span = {\"token\" : \"\",\n",
    "                                  \"label\" : \"O\",\n",
    "                                  \"start_context_char_pos\" : 0,\n",
    "                                  \"end_context_char_pos\" : 0}\n",
    "                    spans_pred_batch_top_1[idx] = [empty_span]\n",
    "            spans_true_batch = [instance['answer'] for instance in instances_batch]\n",
    "            batch_metrics = compute_metrics(spans_true_batch=spans_true_batch,\n",
    "                                            spans_pred_batch_top_1=spans_pred_batch_top_1,\n",
    "                                            prompt_mapper=dataloader.dataset.prompt_mapper)\n",
    "            for metric_name, metric_value in batch_metrics.items():\n",
    "                batch_metrics_list[metric_name].append(metric_value)\n",
    "        avg_loss = np.mean(epoch_loss)\n",
    "        print(f\"Test loss:  {avg_loss}\\n\")\n",
    "\n",
    "        for metric_name, metric_value_list in batch_metrics_list.items():\n",
    "            metric_value = np.mean(metric_value_list)\n",
    "            print(f\"Test {metric_name}: {metric_value}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f779a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, \n",
    "          model,\n",
    "          train_dataloader,\n",
    "          test_dataloader,\n",
    "          optimizer,\n",
    "          device):\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"Epoch [{epoch+1} / {n_epochs}]\\n\")\n",
    "        train_epoch(model=model,\n",
    "                    dataloader=train_dataloader,\n",
    "                    optimizer=optimizer,\n",
    "                    device=device,\n",
    "                    epoch=epoch)\n",
    "        evaluate_epoch(model=model,\n",
    "                       dataloader=test_dataloader,\n",
    "                       device=device,\n",
    "                       epoch=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b170793d",
   "metadata": {},
   "source": [
    "Now we can actually train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1893d353",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_NAME = 'bert-base-uncased'\n",
    "PATH_TO_SAVE_MODEL = 'qaner-legalner-bert-base-uncased3'\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 1e-5 \n",
    "EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d8eb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# bert model\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "model = transformers.AutoModelForQuestionAnswering.from_pretrained(bert_model_name).to(device)\n",
    "\n",
    "tokenizer_kwargs = {\"max_length\": 512,\n",
    "                    \"truncation\": \"only_second\",\n",
    "                    \"padding\": True,\n",
    "                    \"return_tensors\": \"pt\",\n",
    "                    \"return_offsets_mapping\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18cb8e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "collator = Collator(tokenizer=tokenizer, tokenizer_kwargs=tokenizer_kwargs)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               shuffle=True,\n",
    "                                               collate_fn=collator)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                              batch_size=BATCH_SIZE,\n",
    "                                              shuffle=False,\n",
    "                                              collate_fn=collator)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=LEARNING_RATE)\n",
    "\n",
    "train(n_epochs=n_epochs,\n",
    "      model=model,\n",
    "      train_dataloader=train_dataloader,\n",
    "      test_dataloader=test_dataloader,\n",
    "      optimizer=optimizer,\n",
    "      device=device)\n",
    "\n",
    "model.save_pretrained(PATH_TO_SAVE_MODEL)\n",
    "tokenizer.save_pretrained(PATH_TO_SAVE_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87781cb",
   "metadata": {},
   "source": [
    "## 5. Get predictions for unlabeled texts\n",
    "<a id='get_predictions_for_unlabeled_texts'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce837b1-6c41-4416-b580-045579b6b152",
   "metadata": {
    "tags": []
   },
   "source": [
    "After the training is done we could evaluate our model on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff59a1c7-10ea-48d3-a9e9-e49d58fc75e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_spans(txt, first_span, second_span):\n",
    "    interm_tok = '' if first_span['end_context_char_pos'] == second_span['start_context_char_pos'] else txt[first_span['end_context_char_pos']+1]\n",
    "    return {\"token\" : first_span['token']+interm_tok+second_span['token'],\n",
    "            \"start_context_char_pos\" : first_span['start_context_char_pos'],\n",
    "            \"end_context_char_pos\" : second_span['end_context_char_pos'],\n",
    "            \"label\" : first_span['label']}\n",
    "\n",
    "def predict(context, \n",
    "            question,\n",
    "            prompt_mapper,\n",
    "            model,\n",
    "            tokenizer,\n",
    "            tokenizer_kwargs,\n",
    "            n_best_size = 1,\n",
    "            max_answer_length = 100):\n",
    "    inputs = tokenizer([question], [context], **tokenizer_kwargs).to(model.device)\n",
    "    offset_mapping_batch = inputs.pop(\"offset_mapping\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    predicted_spans = get_top_valid_spans(context_list=[context],\n",
    "                                          question_list=[question],\n",
    "                                          prompt_mapper=prompt_mapper,\n",
    "                                          inputs=inputs,\n",
    "                                          outputs=outputs,\n",
    "                                          offset_mapping_batch=offset_mapping_batch,\n",
    "                                          n_best_size=n_best_size,\n",
    "                                          max_answer_length=max_answer_length)[0]\n",
    "    \n",
    "    predicted_spans.sort(key=lambda x: x['start_context_char_pos'])\n",
    "    merged_spans = []\n",
    "    for span in predicted_spans:\n",
    "        if not merged_spans:\n",
    "            merged_spans.append(span)\n",
    "            continue\n",
    "        else:\n",
    "            last_span = merged_spans[-1]\n",
    "        if span['start_context_char_pos']<=last_span['end_context_char_pos']+1 and span['label']==last_span['label']:\n",
    "            merged_spans.pop()\n",
    "            merged_spans.append(merge_spans(context,last_span,span))\n",
    "        else:\n",
    "            merged_spans.append(span)\n",
    "            \n",
    "    return merged_spans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7257574c",
   "metadata": {},
   "source": [
    "Let's upload the unlabeled docs we got from SuperAnnotate and get the predictions for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416756a9-72e1-49a6-b1a6-2e75bbb874cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_texts = []\n",
    "names = []\n",
    "for filename in glob.glob('./unlabeled_sa_docs/*.txt'):\n",
    "    with open(filename) as f:\n",
    "        unlabeled_texts.append(f.read())\n",
    "        names.append(os.path.basename(filename)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c87c1b-736a-4b19-a3c6-a221337bad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_spans = []\n",
    "for text in unlabeled_texts:\n",
    "    for class_tag, class_name in PROMPT_MAPPER.items():\n",
    "        predicted_spans.append(predict(context,\n",
    "                                       question,\n",
    "                                       PROMPT_MAPPER,\n",
    "                                       model,\n",
    "                                       tokenizer,\n",
    "                                       tokenizer_kwargs,\n",
    "                                       n_best_size = 10,\n",
    "                                       max_answer_length = 100))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f87c4c",
   "metadata": {},
   "source": [
    "## 7. Make annotations in SuperAnnotate format\n",
    "<a id='make_annotations_sa_format'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474ab3dc-e27f-4aa9-b23b-7649e26ed782",
   "metadata": {},
   "source": [
    "Based on predictions made by the model we should now create annotations in SuperAnnotate format to be able to upload them to SuperAnnotate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c63a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_annotations = []\n",
    "for spans, name in zip(predicted_spans, names):\n",
    "    entities = []\n",
    "    for span in spans:\n",
    "        entities.append({\"type\": \"entity\",\n",
    "                         \"className\": span['label'],\n",
    "                         \"start\": span[''],\n",
    "                         \"end\": span[''],\n",
    "                         \"attributes\": []})\n",
    "    new_annotations.append({'instances': entities,\n",
    "                            'metadata': {'name' : name}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c9a216",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTATIONS_FOLDER = 'PATH/TO/LOCAL/DIR/' # local folder to store .json files with annotations\n",
    "for annotation in new_annotations:\n",
    "    filename = annotation['metadata']['name']\n",
    "    with open(f'{ANNOTATIONS FOLDER}/{filename}.json','w') as f:\n",
    "        json.dump(js_annotation, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045a14e9",
   "metadata": {},
   "source": [
    "## 8. Upload new annotations to SuperAnnotate platform\n",
    "<a id='upload_new_annotations_to_sa_platform'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921c729b-da73-4139-b338-59843e17ff10",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now we could upload annotations generated on the previous step back to SuperAnnnotate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccdb405-e4f0-4bd1-a89a-1a7cc9831947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(filename):\n",
    "    with open(filename) as f:\n",
    "        data = json.load(f)\n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd96a609-81be-4ec4-9ba5-9f9ac8f84393",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "files = os.listdir(ANNOTATIONS_FOLDER)\n",
    "files_per_step = 500\n",
    "steps = len(files) // files_per_step + 1\n",
    "\n",
    "for step in range(steps):\n",
    "    start = step * files_per_step\n",
    "    end = min((step + 1)* files_per_step, len(files))\n",
    "\n",
    "    batch = [read_json(os.path.join(ANNOTATIONS_FOLDER, f)) for f in files[start: end]]\n",
    "\n",
    "    outputs.append(sa_client.upload_annotations(project=f'{SA_PROJECT_NAME}/unlabeled/', annotations=batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5fa4cc-d03c-41e1-8195-e8f49c0b33b0",
   "metadata": {},
   "source": [
    "Now we can look at unlabeled folder at the SuperAnnotate page and see the predictions made by our model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d6669d",
   "metadata": {},
   "source": [
    "![](../docs/legal-ner/labeled_unlabeled.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92be44fd-6633-4ad9-8563-070f21a9c276",
   "metadata": {},
   "source": [
    "All files in unlabeled folder changed their status."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1be97b",
   "metadata": {},
   "source": [
    "![](../docs/legal-ner/lner_unlabeled_example.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
